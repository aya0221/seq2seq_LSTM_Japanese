{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare the dataset -seq2seq- (データの準備)\n",
    "## settings(各設定)\n",
    "## create Model for training -LSTM- (学習用モデルの構築)\n",
    "## training(学習)\n",
    "## 学習の推移 -誤差の推移を確認-\n",
    "## Create Model for prediction(予測用モデルの作成)\n",
    "## 返答作成用の関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare the dadtaset (使用する文字を開く・足す)\n",
    "・(今回は学習をなるべく簡単にするために、ひらがなとカタカナ、記号のみを使用。  ）\n",
    "・データセットで使われていない文字の入力にも対応するために、全てのひらがなとカタカナ、任意の記号も一応用意。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipykernel in /home/aya/Downloads/enter/lib/python3.8/site-packages (5.5.3)\n",
      "Requirement already satisfied: ipython>=5.0.0 in /home/aya/Downloads/enter/lib/python3.8/site-packages (from ipykernel) (7.21.0)\n",
      "Requirement already satisfied: tornado>=4.2 in /home/aya/Downloads/enter/lib/python3.8/site-packages (from ipykernel) (6.1)\n",
      "Requirement already satisfied: traitlets>=4.1.0 in /home/aya/Downloads/enter/lib/python3.8/site-packages (from ipykernel) (5.0.5)\n",
      "Requirement already satisfied: jupyter-client in /home/aya/Downloads/enter/lib/python3.8/site-packages (from ipykernel) (6.1.7)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/aya/Downloads/enter/lib/python3.8/site-packages (from ipython>=5.0.0->ipykernel) (0.17.2)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/aya/Downloads/enter/lib/python3.8/site-packages (from ipython>=5.0.0->ipykernel) (52.0.0.post20210125)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/aya/Downloads/enter/lib/python3.8/site-packages (from ipython>=5.0.0->ipykernel) (4.8.0)\n",
      "Requirement already satisfied: pickleshare in /home/aya/Downloads/enter/lib/python3.8/site-packages (from ipython>=5.0.0->ipykernel) (0.7.5)\n",
      "Requirement already satisfied: pygments in /home/aya/Downloads/enter/lib/python3.8/site-packages (from ipython>=5.0.0->ipykernel) (2.8.1)\n",
      "Requirement already satisfied: backcall in /home/aya/Downloads/enter/lib/python3.8/site-packages (from ipython>=5.0.0->ipykernel) (0.2.0)\n",
      "Requirement already satisfied: decorator in /home/aya/Downloads/enter/lib/python3.8/site-packages (from ipython>=5.0.0->ipykernel) (4.4.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/aya/Downloads/enter/lib/python3.8/site-packages (from ipython>=5.0.0->ipykernel) (3.0.8)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /home/aya/Downloads/enter/lib/python3.8/site-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel) (0.7.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/aya/Downloads/enter/lib/python3.8/site-packages (from pexpect>4.3->ipython>=5.0.0->ipykernel) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/aya/Downloads/enter/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.0.0->ipykernel) (0.2.5)\n",
      "Requirement already satisfied: ipython-genutils in /home/aya/Downloads/enter/lib/python3.8/site-packages (from traitlets>=4.1.0->ipykernel) (0.2.0)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /home/aya/Downloads/enter/lib/python3.8/site-packages (from jupyter-client->ipykernel) (4.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/aya/Downloads/enter/lib/python3.8/site-packages (from jupyter-client->ipykernel) (2.8.1)\n",
      "Requirement already satisfied: pyzmq>=13 in /home/aya/Downloads/enter/lib/python3.8/site-packages (from jupyter-client->ipykernel) (20.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/aya/Downloads/enter/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client->ipykernel) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t', '\\n', ',', '0', '1', '2', '3', '4', '5', '6', '7', '8', 'A', 'C', 'E', 'K', 'O', 'P', 'S', 'U', 'X', 'a', 'c', 'e', 'h', 'm', 'p', 'r', 's', '、', '。', 'ぁ', 'あ', 'ぃ', 'い', 'ぅ', 'う', 'ぇ', 'え', 'ぉ', 'お', 'か', 'が', 'き', 'ぎ', 'く', 'ぐ', 'け', 'げ', 'こ', 'ご', 'さ', 'ざ', 'し', 'じ', 'す', 'ず', 'せ', 'ぜ', 'そ', 'ぞ', 'た', 'だ', 'ち', 'ぢ', 'っ', 'つ', 'づ', 'て', 'で', 'と', 'ど', 'な', 'に', 'ぬ', 'ね', 'の', 'は', 'ば', 'ぱ', 'ひ', 'び', 'ぴ', 'ふ', 'ぶ', 'ぷ', 'へ', 'べ', 'ぺ', 'ほ', 'ぼ', 'ぽ', 'ま', 'み', 'む', 'め', 'も', 'ゃ', 'や', 'ゅ', 'ゆ', 'ょ', 'よ', 'ら', 'り', 'る', 'れ', 'ろ', 'ゎ', 'わ', 'ゐ', 'ゑ', 'を', 'ん', 'ァ', 'ア', 'ィ', 'イ', 'ゥ', 'ウ', 'ェ', 'エ', 'ォ', 'オ', 'カ', 'ガ', 'キ', 'ギ', 'ク', 'グ', 'ケ', 'ゲ', 'コ', 'ゴ', 'サ', 'ザ', 'シ', 'ジ', 'ス', 'ズ', 'セ', 'ゼ', 'ソ', 'ゾ', 'タ', 'ダ', 'チ', 'ヂ', 'ッ', 'ツ', 'ヅ', 'テ', 'デ', 'ト', 'ド', 'ナ', 'ニ', 'ヌ', 'ネ', 'ノ', 'ハ', 'バ', 'パ', 'ヒ', 'ビ', 'ピ', 'フ', 'ブ', 'プ', 'ヘ', 'ベ', 'ペ', 'ホ', 'ボ', 'ポ', 'マ', 'ミ', 'ム', 'メ', 'モ', 'ャ', 'ヤ', 'ュ', 'ユ', 'ョ', 'ヨ', 'ラ', 'リ', 'ル', 'レ', 'ロ', 'ヮ', 'ワ', 'ヰ', 'ヱ', 'ヲ', 'ン', 'ヴ', '・', 'ー', '！', '０', '１', '２', '３', '４', '５', '６', '７', '８', '：', '？', 'ｃ', 'ｍ', '～']\n"
     ]
    }
   ],
   "source": [
    "import pickle #保存のためにpickleをimportする\n",
    "\n",
    "#全てのひらがなとカタカナを文字列として用意する\n",
    "hiragana = \"ぁあぃいぅうぇえぉおかがきぎくぐけげこごさざしじすずせぜそぞ\\\n",
    "ただちぢっつづてでとどなにぬねのはばぱひびぴふぶぷへべぺほぼぽ\\\n",
    "まみむめもゃやゅゆょよらりるれろゎわゐゑをん\"\n",
    "\n",
    "katakana = \"ァアィイゥウェエォオカガキギクグケゲコゴサザシジスズセゼソゾ\\\n",
    "タダチヂッツヅテデトドナニヌネノハバパヒビピフブプヘベペホボポ\\\n",
    "マミムメモャヤュユョヨラリルレロヮワヰヱヲンヴ\"\n",
    "\n",
    "#この２つの文字列を加えてcharsとしておく\n",
    "chars = hiragana + katakana\n",
    "\n",
    "#前処理して保存したデータを開く\n",
    "with open(\"kana_pepper.txt\", mode=\"r\", encoding=\"utf-8\") as f:  \n",
    "    text = f.read()\n",
    "\n",
    "#テキストデータの各文字でcharsに含まれていないものはcharsに加える    \n",
    "for char in text:  # ひらがな、カタカナ以外でコーパスに使われている文字を追加\n",
    "    if char not in chars:\n",
    "        chars += char\n",
    "        \n",
    "chars += \"\\t\\n\"  # タブと改行を追加\n",
    "        \n",
    "chars_list = sorted(list(chars))  # 文字列をリストに変換してソートする\n",
    "print(chars_list)\n",
    "\n",
    "with open(\"kana_chars.pickle\", mode=\"wb\") as f:  # pickleで保存\n",
    "    pickle.dump(chars_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one-hot表現（文字のベクトル化）\n",
    "## preparing data\n",
    "encoderへの入力、decoderへの入力、decoderの正解を作成\n",
    "各文章はそれぞれ長さが違うので、文章の終了後は全て0のベクトルで埋める。\n",
    "学習効率のとため、長すぎる文章はカット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1977, 128, 215)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# インデックスと文字で辞書を作成\n",
    "char_indices = {}  # 文字がキーでインデックスが値\n",
    "for i, char in enumerate(chars_list):\n",
    "    char_indices[char] = i\n",
    "indices_char = {}  # インデックスがキーで文字が値\n",
    "for i, char in enumerate(chars_list):\n",
    "    indices_char[i] = char\n",
    "    \n",
    "seperator = \"。\"\n",
    "sentence_list = text.split(seperator) \n",
    "sentence_list.pop() \n",
    "sentence_list = [x+seperator for x in sentence_list]\n",
    "\n",
    "max_sentence_length = 128  # 文章の最大長さ。これより長い文章はカットされる。\n",
    "sentence_list = [sentence for sentence in sentence_list if len(sentence) <= max_sentence_length]  # 長すぎる文章のカット\n",
    "\n",
    "n_char = len(chars_list)  # 文字の種類の数\n",
    "n_sample = len(sentence_list) - 1  # サンプル数\n",
    "\n",
    "x_sentences = []  # 入力の文章\n",
    "t_sentences = []  # 正解の文章\n",
    "for i in range(n_sample):\n",
    "    x_sentences.append(sentence_list[i])\n",
    "    t_sentences.append(\"\\t\" + sentence_list[i+1] + \"\\n\")  # 正解は先頭にタブ、末尾に改行を加える\n",
    "max_length_x = max_sentence_length  # 入力文章の最大長さ\n",
    "max_length_t = max_sentence_length + 2  # 正解文章の最大長さ\n",
    "\n",
    "x_encoder = np.zeros((n_sample, max_length_x, n_char), dtype=np.bool)  # encoderへの入力\n",
    "x_decoder = np.zeros((n_sample, max_length_t, n_char), dtype=np.bool)  # decoderへの入力\n",
    "t_decoder = np.zeros((n_sample, max_length_t, n_char), dtype=np.bool)  # decoderの正解\n",
    "\n",
    "for i in range(n_sample):\n",
    "    x_sentence = x_sentences[i]\n",
    "    t_sentence = t_sentences[i]\n",
    "    for j, char in enumerate(x_sentence):\n",
    "        x_encoder[i, j, char_indices[char]] = 1  # encoderへの入力をone-hot表現で表す\n",
    "    for j, char in enumerate(t_sentence):\n",
    "        x_decoder[i, j, char_indices[char]] = 1  # decoderへの入力をone-hot表現で表す\n",
    "        if j > 0:  # 正解は入力より1つ前の時刻のものにする\n",
    "            t_decoder[i, j-1, char_indices[char]] = 1\n",
    "            \n",
    "print(x_encoder.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## settings(各設定)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 50 #実験なので少なめに設定(最低でも1000はほしい！)\n",
    "n_mid = 256  # 中間層のニューロン数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create Model for training -LSTM- (学習用モデルの構築)\n",
    "学習用のSeq2Seqモデルの構築\n",
    "・GRUを使用。  \n",
    "・入力の直後にMasking層を挟む（ことにより、全ての要素が0であるベクトルの入力は無視される。）  \n",
    "・GRU層にはdropoutを設定し、ニューロンをランダムに無効にすることで過学習対策をする。  \n",
    "（＊過学習:モデルが訓練データに過剰に適応してしまい、未知のデータに対して機能しなくなってしまうこと。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 215)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None, 215)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking (Masking)               (None, None, 215)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, None, 215)    0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru (GRU)                       [(None, 256), (None, 363264      masking[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     [(None, None, 256),  363264      masking_1[0][0]                  \n",
      "                                                                 gru[0][1]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 215)    55255       gru_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 781,783\n",
      "Trainable params: 781,783\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, GRU, Input, Masking\n",
    "\n",
    "encoder_input = Input(shape=(None, n_char))\n",
    "encoder_mask = Masking(mask_value=0)  # 全ての要素が0であるベクトルの入力は無視する\n",
    "encoder_masked = encoder_mask(encoder_input)\n",
    "#encoder_lstm(今回はGRU)を設定\n",
    "encoder_lstm = GRU(n_mid, dropout=0.2, recurrent_dropout=0.2, return_state=True)  # dropoutを設定し、ニューロンをランダムに無効にする\n",
    "encoder_output, encoder_state_h = encoder_lstm(encoder_masked)\n",
    "\n",
    "decoder_input = Input(shape=(None, n_char))\n",
    "decoder_mask = Masking(mask_value=0)  # 全ての要素が0であるベクトルの入力は無視する\n",
    "decoder_masked = decoder_mask(decoder_input)\n",
    "decoder_lstm = GRU(n_mid, dropout=0.2, recurrent_dropout=0.2, return_sequences=True, return_state=True)  # dropoutを設定\n",
    "decoder_output, _ = decoder_lstm(decoder_masked, initial_state=encoder_state_h)  # encoderの状態を初期状態にする\n",
    "decoder_dense = Dense(n_char, activation='softmax')\n",
    "decoder_output = decoder_dense(decoder_output)\n",
    "\n",
    "model = Model([encoder_input, decoder_input], decoder_output)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training(学習)\n",
    "先程構築したSeq2Seqのモデルを使って、学習を行う。  \n",
    "・今回は、**早期終了**を設定する。  \n",
    "→コールバックにEarlyStoppingを設定することで、学習を自動で終了させることができる。  \n",
    "・誤差に改善が見られなくなってからpatianceで設定したエポック数が経過すると、学習は終了する。  \n",
    "→patienceの値を小さくすると、学習時間が短くなる。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "27/56 [=============>................] - ETA: 1:22 - loss: 0.7404"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping \n",
    "\n",
    "# val_lossに改善が見られなくなってから、30エポックで学習は終了\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=30) \n",
    "\n",
    "history = model.fit([x_encoder, x_decoder], t_decoder,\n",
    "                     batch_size=batch_size,\n",
    "                     epochs=epochs,\n",
    "                     validation_split=0.1,  # 10%は検証用\n",
    "                     callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習の推移 -誤差の推移を確認-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.plot(np.arange(len(loss)), loss)\n",
    "plt.plot(np.arange(len(val_loss)), val_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model for prediction(予測用モデルの作成)\n",
    "学習済みのオブジェクトから、encoder、decoderのモデルを個別に構築する。    \n",
    "encoderは入力を受け取って状態を返し、decoderは入力と状態を受け取って出力と状態を返すようにする。  \n",
    "構築したモデルは保存する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoderのモデル\n",
    "encoder_model = Model(encoder_input, encoder_state_h)\n",
    "\n",
    "# decoderのモデル\n",
    "decoder_state_in_h = Input(shape=(n_mid,))\n",
    "decoder_state_in = [decoder_state_in_h]\n",
    "\n",
    "decoder_output, decoder_state_h = decoder_lstm(decoder_input,\n",
    "                                               initial_state=decoder_state_in_h)\n",
    "decoder_output = decoder_dense(decoder_output)\n",
    "\n",
    "decoder_model = Model([decoder_input] + decoder_state_in,\n",
    "                      [decoder_output, decoder_state_h])\n",
    "\n",
    "# モデルの保存\n",
    "encoder_model.save('encoder_model.h5')\n",
    "decoder_model.save('decoder_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 返答作成用の関数\n",
    "・入力を出力に変換し、返答を作成するための関数を設定する。  \n",
    "・decoderでは、各時刻ごとに予測を行い、出力と状態を次の時刻に渡す。  \n",
    "・decoderの出力を確率として捉え、その確率に従ってサンプリングを行うので、実行するたびにやや異なる文章が生成されるように。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond(input_data, beta=5):\n",
    "    state_value = encoder_model.predict(input_data)\n",
    "    y_decoder = np.zeros((1, 1, n_char))  # decoderの出力を格納する配列\n",
    "    y_decoder[0][0][char_indices[\"\\t\"]] = 1  # decoderの最初の入力はタブ。one-hot表現にする。\n",
    "\n",
    "    respond_sentence = \"\"  # 返答の文字列\n",
    "    while True:\n",
    "        y, h = decoder_model.predict([y_decoder, state_value])\n",
    "        p_power = y[0][0] ** beta  # 確率分布の調整\n",
    "        next_index = np.random.choice(len(p_power), p=p_power/np.sum(p_power)) \n",
    "        next_char = indices_char[next_index]  # 次の文字\n",
    "\n",
    "        if (next_char == \"\\n\" or len(respond_sentence) >= max_length_x):\n",
    "            break  # 次の文字が改行のとき、もしくは最大文字数を超えたときは終了\n",
    "            \n",
    "        respond_sentence += next_char\n",
    "        y_decoder = np.zeros((1, 1, n_char))  # 次の時刻の入力\n",
    "        y_decoder[0][0][next_index] = 1\n",
    "\n",
    "        state_value = h  # 次の時刻の状態\n",
    "\n",
    "    return respond_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 動作の確認\n",
    "訓練データの最初のrange(文字数)を使って、どのような返答が返ってくるかを確かめる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20000):  \n",
    "    x_in = x_encoder[i:i+1]  # 入力\n",
    "    responce = respond(x_in)  # 返答\n",
    "    print(\"Input:\", x_sentences[i])\n",
    "    print(\"Response:\", responce)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trasnformer\n",
    "attention:文章中のどの単語に注目すればいいかを表すスコア\n",
    "query, key, valueの３つのベクトルで計算される。\n",
    "入力文章から各単語のqueryを作る\n",
    "memoryから全結合層に入れてkeyを作る。\n",
    "そのqueryとkeyの内積ベクトルを取ることでinput とmemoryの関連度を測定\n",
    "その後softmax関数に入れて、値を０−１で確率を取る。\n",
    "関連度をsoftmax関数に入れたものがattention weightになる。\n",
    "・memoryの各単語を表す埋め込みベクトル\n",
    "で、　attention weightとvalueの間で内積をとる\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}